{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "topic_modeling_venv",
   "display_name": "topic_modeling_venv",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import spacy\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import en_core_web_lg\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#CONSIDERING ONLY LINES STARTING WITH \"-\" or \"+\" AND REMOVING LINES STARTING WITH \"NOT_ALLOWED\" WORDS\n",
    "diffprova = open(\"filediffvero.diff\", \"r\")\n",
    "output_file = open(\"filediffsolocommit.diff\",\"w\")\n",
    "not_allowed=[\"---\",\"+++\",\"-/*\",\"- *\",\"import\",\"-#\",\"+/*\",\"+ *\",\"+#\",\"+package\",\"-package\",\"@\"]\n",
    "for line in diffprova.readlines():\n",
    "    if (line.startswith('-') or line.startswith('+')):\n",
    "            if not any(not_allowed in line for not_allowed in not_allowed):\n",
    "                #way to do multiple sub in a single sentence\n",
    "                rep = {\"Set<\": \" \", \"Pair<\": \" \",\"Field>>>\": \" \"} \n",
    "                rep = dict((re.escape(k), v) for k, v in rep.items()) \n",
    "                pattern = re.compile(\"|\".join(rep.keys()))\n",
    "                line_no_set_pair = pattern.sub(lambda m: rep[re.escape(m.group(0))], line)\n",
    "                line_no_html_tags= re.sub(r'<.*?>', '',line_no_set_pair)\n",
    "                #REMOVE EMPTY LINES\n",
    "                if (len(line_no_html_tags.split())> 1):\n",
    "                    #ASSUMING THAT THE FIRST CHAR IS ALWAYS A \"+\" OR \"-\" WE'RE REMOVING IT\n",
    "                    cleaned_line = line_no_html_tags[1:]\n",
    "                    #REMOVING LEADING SPACES\n",
    "                    cleaned_line = cleaned_line.strip()\n",
    "                    output_file.write(cleaned_line)\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = open(\"filediffsolocommit.diff\",\"r\")\n",
    "#print(output_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_prova_vera= output_file.read().encode(\"utf-8\")\n",
    "#print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp= spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# My list of stop words.\n",
    "stop_word = open(\"stop_word.txt\", \"r\")\n",
    "stop_list = stop_word.readline().split(\",\")\n",
    "# Updates spaCy's default stop words list with my additional words. \n",
    "nlp.Defaults.stop_words.update(stop_list)\n",
    "\n",
    "# Iterates over the words in the stop words list and resets the \"is_stop\" flag.\n",
    "for word in STOP_WORDS:\n",
    "    lexeme = nlp.vocab[word]\n",
    "    lexeme.is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def camel_case_split(token):\n",
    "    '''\n",
    "    Splits a CamelCase token into a list of tokens,\n",
    "\n",
    "    Input:\n",
    "        token (str): the token that should be split if it is in CamelCase\n",
    "\n",
    "    Returns:\n",
    "        None: if the token is not in CamelCase\n",
    "        list: 'CamelCase' --> ['CamelCase', 'camel', 'case']\n",
    "    '''\n",
    "    if type(token) != str:\n",
    "        raise TypeError('The provided token should be a str data type but is of type {}.'.format(type(token)))\n",
    "\n",
    "    matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', token)\n",
    "    result = [m.group(0).lower() for m in matches]\n",
    "    if len(result) == 1:\n",
    "        return\n",
    "    return [token] + result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snake_case_split(token):\n",
    "    '''\n",
    "    Splits a snake_case token into a list of tokens,\n",
    "\n",
    "    Input:\n",
    "        token (str): the token that should be split if it is in CamelCase\n",
    "\n",
    "    Returns:\n",
    "        None: if the token is not in CamelCase\n",
    "        list: 'CamelCase' --> ['CamelCase', 'camel', 'case']\n",
    "    '''\n",
    "    if type(token) != str:\n",
    "        raise TypeError('The provided token should be a str data type but is of type {}.'.format(type(token)))\n",
    "\n",
    "    result = token.split('_')\n",
    "    if len(result) == 1:\n",
    "        return \n",
    "    return [token] + result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_case_split(token):\n",
    "    '''\n",
    "    Splits a dot.case token into a list of tokens,\n",
    "\n",
    "    Input:\n",
    "        token (str): the token that should be split if it is in dot.case\n",
    "\n",
    "    Returns:\n",
    "        None: if the token is not in dot.case\n",
    "        list: 'dot.case' --> ['dot.case', 'dot', 'case']\n",
    "    '''\n",
    "    if type(token) != str:\n",
    "        raise TypeError('The provided token should be a str data type but is of type {}.'.format(type(token)))\n",
    "\n",
    "    result = token.split('.')\n",
    "    if len(result) == 1:\n",
    "        return \n",
    "    return [token] + result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_doc(doc):\n",
    "    if type(doc) != spacy.tokens.doc.Doc:\n",
    "        raise TypeError(\"The document should be a spacy.tokens.doc.Doc, which is created by means of nlp(\")\n",
    "    \n",
    "    tokens = [token for token in doc if token.is_punct == False and token.is_stop == False and any(char for char in token.text if char.isalpha()) and len(token) > 1] #token.pos_ in ['VERB', 'NOUN', 'PROPN', 'ADJ'] and \n",
    "    result = list()\n",
    "    for token in tokens:\n",
    "        if camel_case_split(token.text):\n",
    "            result += [camel_case_token.lemma_ for camel_case_token in nlp(' '.join(camel_case_split(token.text)))]\n",
    "        elif snake_case_split(token.text):\n",
    "            result += [snake_case_token.lemma_ for snake_case_token in nlp(' '.join(snake_case_split(token.text)))]\n",
    "        elif dot_case_split(token.text):\n",
    "            result += [dot_case_token.lemma_ for dot_case_token in nlp(' '.join(dot_case_split(token.text)))]\n",
    "\n",
    "        else:\n",
    "            result.append(str(token.lemma_).lower())\n",
    "\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_into_chunks(text, chunk_size=1000):\n",
    "    '''\n",
    "    Yield successive n-sized chunks from list.\n",
    "    '''\n",
    "    if type(text) == list:\n",
    "        text = ' '.join(text)\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpler_filter_text(text):\n",
    "    ''' Similar to filter_text but without options:\n",
    "            will be lemmatized and returned as a string\n",
    "    '''\n",
    "\n",
    "    # when a list is provided concatenate it into a string\n",
    "    if type(text) == list:\n",
    "        text = ' '.join([str(line) for line in text])\n",
    "\n",
    "    # filter text, needs to be in chunks due to spacy maximum of 1000000 characters\n",
    "    return ' '.join([filter_doc(nlp(chunk)) for chunk in text_into_chunks(text, chunk_size = 10000)]).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVING ALL SNAKE,CAMEL,DOT WORDS\n",
    "processed_commit= simpler_filter_text(str(diff_prova_vera))\n",
    "\n",
    "#REMOVING BREAKLINE\n",
    "processed_commit= processed_commit.replace(r'/(\\r\\n|\\n|\\r)/gm', \"\")\n",
    "\n",
    "#REMOVING SPECIAL CHARACTER\n",
    "#processed_commit=re.sub('/^[a-z\\d\\-_\\s]+$/i', '', processed_commit)\n",
    "processed_commit=re.sub('[^a-zA-Z \\n\\.]', ' ', processed_commit) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(doc):\n",
    "    # This takes in a doc of tokens from the NER and lemmatizes them. \n",
    "    # Pronouns (like \"I\" and \"you\" get lemmatized to '-PRON-', so I'm removing those.\n",
    "    doc = [token.lemma_ for token in doc if token.lemma_ != '-PRON-']\n",
    "    doc = u' '.join(doc)\n",
    "    return nlp.make_doc(doc)\n",
    "    \n",
    "def remove_stopwords(doc):\n",
    "    # This will remove stopwords and punctuation.\n",
    "    # Use token.text to return strings, which we'll need for Gensim.\n",
    "    doc = [token.text for token in doc if token.is_stop != True and token.is_punct != True]\n",
    "    return doc\n",
    "\n",
    "# The add_pipe function appends our functions to the default pipeline.\n",
    "nlp.add_pipe(lemmatizer,name='lemmatizer',after='ner')\n",
    "nlp.add_pipe(remove_stopwords, name=\"stopwords\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc_list = []\n",
    "# Iterates through each article in the corpus.\n",
    "#for doc in tqdm(newest_doc):\n",
    "    # Passes that article through the pipeline and adds to a new list.\n",
    "    #pr = nlp(doc)\n",
    "    #doc_list.append(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = []\n",
    "pr=nlp(str(processed_commit))\n",
    "doc_list.append(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates, which is a mapping of word IDs to words.\n",
    "words = corpora.Dictionary(doc_list)\n",
    "\n",
    "# Turns each document into a bag of words.\n",
    "corpus = [words.doc2bow(doc) for doc in doc_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=words,\n",
    "                                           num_topics=1, \n",
    "                                           random_state=2,\n",
    "                                           update_every=1,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(0,\n  '0.019*\"template\" + 0.017*\"realm\" + 0.017*\"jexl\" + 0.015*\"arg\" + '\n  '0.015*\".get\" + 0.014*\"password\" + 0.014*\"reset\" + 0.013*\"html\" + '\n  '0.012*\"evaluate\" + 0.012*\"user\" + 0.011*\"super.get\" + 0.011*\"stre\" + '\n  '0.011*\"request\" + 0.010*\"username\" + 0.010*\"uberspect\" + 0.010*\"dao\" + '\n  '0.010*\"super.getmethod\" + 0.010*\"http\" + 0.010*\"map\" + 0.010*\"property\" + '\n  '0.010*\"event\" + 0.010*\"localhost\" + 0.009*\"confirmpasswordreset\" + '\n  '0.008*\"isp\" + 0.008*\"list\" + 0.008*\"token\" + 0.008*\"dao.find\" + 0.008*\"ctx\" '\n  '+ 0.008*\"true\" + 0.008*\"body\" + 0.007*\"user.get\" + 0.007*\"plain\" + '\n  '0.007*\"syncope\" + 0.007*\"set\" + 0.007*\"email\" + 0.007*\"uid\" + '\n  '0.007*\"methods.contain\" + 0.007*\"collection\" + 0.007*\"link\" + 0.006*\"conn\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda_model.print_topics(num_words=40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}